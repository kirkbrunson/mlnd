import random
import math
from environment import Agent, Environment
from planner import RoutePlanner
from simulator import Simulator


def state_key(state):
    state[1].update({'goal': state[0]})
    features = ['goal', 'light', 'oncoming']
    dirs = {'None': 0, 'forward': 1, 'right': 2, 'left': 3, 'red': 4, 'green': 5}
    goal, inputs, deadline = state
    return ''.join(map(str, [dirs[str(inputs.get(f))] for f in features]))


class LearningAgent(Agent):

    """An agent that learns to drive in the Smartcab world"""

    def __init__(self, env, learning=False, epsilon=1.0, alpha=0.5):
        # Set the agent in the evironment
        super(LearningAgent, self).__init__(env)
        self.planner = RoutePlanner(self.env, self)
        self.valid_actions = self.env.valid_actions
        self.learning = learning
        self.Q = dict()
        self.epsilon = epsilon
        self.alpha = alpha
        self.t = 1

    def reset(self, destination=None, testing=False):
        """ The reset function is called at the beginning of each trial.
            'testing' is set to True if testing trials are being used
            once training trials have completed. """

        self.planner.route_to(destination)

        # If 'testing' is True, set epsilon and alpha to 0
        if testing:
            self.epsilon = 0
            self.alpha = 0
        elif self.epsilon > 0.0 and self.alpha > 0.0:
            self.epsilon = math.cos(self.alpha*self.t)
            # self.epsilon = math.exp(-self.alpha*self.t)

    def build_state(self):
        """ The build_state function is called when the agent requests data from the 
            environment. The next waypoint, the intersection inputs, and the deadline 
            are all features available to the agent. """

        waypoint = self.planner.next_waypoint()  # The next waypoint
        # Visual input - intersection light and traffic
        inputs = self.env.sense(self)
        deadline = self.env.get_deadline(self)  # Remaining deadline
        state = (waypoint, inputs, deadline)
        return state


    def get_maxQ(self, state):
        """ The get_max_Q function is called when the agent is asked to find the
            maximum Q-value of all actions based on the 'state' the smartcab is in. """
        sk = state_key(state)
        return max(self.Q[sk], key=self.Q[sk].get)


    def createQ(self, state):
        """ The createQ function is called when a state is generated by the agent. """
        sk = state_key(state)
        if self.learning and sk not in self.Q:
            # add state to Q with inits
            self.Q[sk] = {self.valid_actions[i]: 0 for i in range(len(self.valid_actions))}


    def choose_action(self, state):
        self.state = state
        self.next_waypoint = self.planner.next_waypoint()

        if random.random() < self.epsilon:
            action = self.valid_actions[random.randint(0, 3)]
        else:
            action = self.get_maxQ(state)
        return action


    def learn(self, state, action, reward):
        if self.learning:
            sk = state_key(state)
            learning = math.sqrt(self.alpha)
            self.Q[sk][action] = (1.-learning)*self.Q[sk][action] + learning*reward


    def update(self):
        state = self.build_state()          # Get current state
        self.createQ(state)                 # Create 'state' in Q-table
        action = self.choose_action(state)  # Choose an action
        reward = self.env.act(self, action)  # Receive a reward
        self.learn(state, action, reward)   # Q-learn
        self.t += 1

def run():
    """Driving function Press ESC to close simulation or [SPACE] to pause"""

    # Env flags:
    # verbose
    # num_dummies - discrete number of dummy agents in the environment, default: 100
    # grid_size   - discrete number of intersections (columns, rows), default: (8, 6)
    env = Environment()
    agent = env.create_agent(LearningAgent, learning=True, alpha=0.0009, epsilon=1)
    env.set_primary_agent(agent, enforce_deadline=True)
    sim = Simulator(env, update_delay=0.01, log_metrics=True, optimized=True, display=True)
    sim.run(n_test=10, tolerance=0.001)


if __name__ == '__main__':
    run()
